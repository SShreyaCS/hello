{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1eMKhD8Jg3O",
        "outputId": "726aa19a-2f72-4572-b153-01b01bb794bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp310-cp310-win_amd64.whl (51.0 MB)\n",
            "     ---------------------------------------- 0.0/51.0 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.4/51.0 MB 11.6 MB/s eta 0:00:05\n",
            "      --------------------------------------- 1.2/51.0 MB 15.4 MB/s eta 0:00:04\n",
            "     - -------------------------------------- 2.5/51.0 MB 22.7 MB/s eta 0:00:03\n",
            "     -- ------------------------------------- 3.7/51.0 MB 23.6 MB/s eta 0:00:03\n",
            "     ---- ----------------------------------- 5.6/51.0 MB 25.3 MB/s eta 0:00:02\n",
            "     ----- ---------------------------------- 6.9/51.0 MB 25.9 MB/s eta 0:00:02\n",
            "     ------ --------------------------------- 8.7/51.0 MB 27.7 MB/s eta 0:00:02\n",
            "     ------- ------------------------------- 10.0/51.0 MB 26.6 MB/s eta 0:00:02\n",
            "     -------- ------------------------------ 11.4/51.0 MB 31.2 MB/s eta 0:00:02\n",
            "     --------- ----------------------------- 13.0/51.0 MB 32.7 MB/s eta 0:00:02\n",
            "     ---------- ---------------------------- 14.4/51.0 MB 32.7 MB/s eta 0:00:02\n",
            "     ----------- --------------------------- 15.1/51.0 MB 31.2 MB/s eta 0:00:02\n",
            "     ------------ -------------------------- 16.3/51.0 MB 28.4 MB/s eta 0:00:02\n",
            "     ------------- ------------------------- 17.3/51.0 MB 28.5 MB/s eta 0:00:02\n",
            "     -------------- ------------------------ 18.5/51.0 MB 28.4 MB/s eta 0:00:02\n",
            "     --------------- ----------------------- 20.4/51.0 MB 29.7 MB/s eta 0:00:02\n",
            "     ---------------- ---------------------- 22.0/51.0 MB 28.5 MB/s eta 0:00:02\n",
            "     ----------------- --------------------- 23.5/51.0 MB 29.8 MB/s eta 0:00:01\n",
            "     ------------------ -------------------- 24.7/51.0 MB 29.7 MB/s eta 0:00:01\n",
            "     ------------------- ------------------- 25.9/51.0 MB 28.4 MB/s eta 0:00:01\n",
            "     -------------------- ------------------ 27.0/51.0 MB 29.7 MB/s eta 0:00:01\n",
            "     ---------------------- ---------------- 28.9/51.0 MB 31.2 MB/s eta 0:00:01\n",
            "     ----------------------- --------------- 30.8/51.0 MB 31.2 MB/s eta 0:00:01\n",
            "     ------------------------ -------------- 32.5/51.0 MB 32.7 MB/s eta 0:00:01\n",
            "     -------------------------- ------------ 34.2/51.0 MB 34.6 MB/s eta 0:00:01\n",
            "     --------------------------- ----------- 35.6/51.0 MB 32.7 MB/s eta 0:00:01\n",
            "     ---------------------------- ---------- 37.2/51.0 MB 34.4 MB/s eta 0:00:01\n",
            "     ----------------------------- --------- 38.7/51.0 MB 34.4 MB/s eta 0:00:01\n",
            "     ------------------------------ -------- 40.2/51.0 MB 32.7 MB/s eta 0:00:01\n",
            "     ------------------------------- ------- 41.4/51.0 MB 32.7 MB/s eta 0:00:01\n",
            "     -------------------------------- ------ 42.7/51.0 MB 31.2 MB/s eta 0:00:01\n",
            "     --------------------------------- ----- 43.5/51.0 MB 29.7 MB/s eta 0:00:01\n",
            "     ---------------------------------- ---- 44.7/51.0 MB 29.8 MB/s eta 0:00:01\n",
            "     ----------------------------------- --- 46.2/51.0 MB 29.8 MB/s eta 0:00:01\n",
            "     ------------------------------------ -- 47.9/51.0 MB 29.7 MB/s eta 0:00:01\n",
            "     ------------------------------------- - 49.4/51.0 MB 29.7 MB/s eta 0:00:01\n",
            "     --------------------------------------  51.0/51.0 MB 29.7 MB/s eta 0:00:01\n",
            "     --------------------------------------  51.0/51.0 MB 29.7 MB/s eta 0:00:01\n",
            "     --------------------------------------- 51.0/51.0 MB 21.8 MB/s eta 0:00:00\n",
            "Collecting albumentations==1.1.0\n",
            "  Downloading albumentations-1.1.0-py3-none-any.whl (102 kB)\n",
            "     ---------------------------------------- 0.0/102.4 kB ? eta -:--:--\n",
            "     -------------------------------------- 102.4/102.4 kB 5.8 MB/s eta 0:00:00\n",
            "Collecting timm\n",
            "  Downloading timm-1.0.22-py3-none-any.whl (2.5 MB)\n",
            "     ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
            "     -------------------- ------------------- 1.3/2.5 MB 41.0 MB/s eta 0:00:01\n",
            "     ---------------------------------------  2.5/2.5 MB 31.9 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 2.5/2.5 MB 26.8 MB/s eta 0:00:00\n",
            "Collecting PyYAML\n",
            "  Downloading pyyaml-6.0.3-cp310-cp310-win_amd64.whl (158 kB)\n",
            "     ---------------------------------------- 0.0/158.6 kB ? eta -:--:--\n",
            "     -------------------------------------- 158.6/158.6 kB 9.9 MB/s eta 0:00:00\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-win_amd64.whl (38.9 MB)\n",
            "     ---------------------------------------- 0.0/38.9 MB ? eta -:--:--\n",
            "     - -------------------------------------- 1.5/38.9 MB 47.2 MB/s eta 0:00:01\n",
            "     --- ------------------------------------ 3.1/38.9 MB 32.9 MB/s eta 0:00:02\n",
            "     ---- ----------------------------------- 4.8/38.9 MB 33.9 MB/s eta 0:00:02\n",
            "     ------ --------------------------------- 6.0/38.9 MB 29.6 MB/s eta 0:00:02\n",
            "     ------- -------------------------------- 7.7/38.9 MB 32.9 MB/s eta 0:00:01\n",
            "     --------- ------------------------------ 8.9/38.9 MB 30.1 MB/s eta 0:00:01\n",
            "     ---------- ---------------------------- 10.8/38.9 MB 31.2 MB/s eta 0:00:01\n",
            "     ------------ -------------------------- 12.2/38.9 MB 29.7 MB/s eta 0:00:01\n",
            "     ------------- ------------------------- 13.2/38.9 MB 29.7 MB/s eta 0:00:01\n",
            "     -------------- ------------------------ 14.8/38.9 MB 31.2 MB/s eta 0:00:01\n",
            "     --------------- ----------------------- 15.6/38.9 MB 29.7 MB/s eta 0:00:01\n",
            "     ----------------- --------------------- 17.3/38.9 MB 31.2 MB/s eta 0:00:01\n",
            "     ------------------ -------------------- 18.5/38.9 MB 29.7 MB/s eta 0:00:01\n",
            "     -------------------- ------------------ 20.1/38.9 MB 32.8 MB/s eta 0:00:01\n",
            "     --------------------- ----------------- 21.8/38.9 MB 31.2 MB/s eta 0:00:01\n",
            "     ----------------------- --------------- 23.5/38.9 MB 34.4 MB/s eta 0:00:01\n",
            "     ------------------------- ------------- 25.2/38.9 MB 32.8 MB/s eta 0:00:01\n",
            "     -------------------------- ------------ 26.6/38.9 MB 32.7 MB/s eta 0:00:01\n",
            "     ---------------------------- ---------- 28.5/38.9 MB 32.8 MB/s eta 0:00:01\n",
            "     ------------------------------ -------- 30.1/38.9 MB 32.7 MB/s eta 0:00:01\n",
            "     ------------------------------- ------- 31.5/38.9 MB 32.7 MB/s eta 0:00:01\n",
            "     --------------------------------- ----- 33.0/38.9 MB 32.7 MB/s eta 0:00:01\n",
            "     ---------------------------------- ---- 34.7/38.9 MB 31.2 MB/s eta 0:00:01\n",
            "     ------------------------------------ -- 36.3/38.9 MB 32.8 MB/s eta 0:00:01\n",
            "     --------------------------------------  38.0/38.9 MB 31.2 MB/s eta 0:00:01\n",
            "     --------------------------------------  38.9/38.9 MB 31.2 MB/s eta 0:00:01\n",
            "     --------------------------------------  38.9/38.9 MB 31.2 MB/s eta 0:00:01\n",
            "     --------------------------------------- 38.9/38.9 MB 25.2 MB/s eta 0:00:00\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
            "     ---------------------------------------- 0.0/41.3 MB ? eta -:--:--\n",
            "     - -------------------------------------- 1.3/41.3 MB 28.3 MB/s eta 0:00:02\n",
            "     -- ------------------------------------- 2.8/41.3 MB 30.3 MB/s eta 0:00:02\n",
            "     ---- ----------------------------------- 4.5/41.3 MB 31.8 MB/s eta 0:00:02\n",
            "     ----- ---------------------------------- 5.8/41.3 MB 31.0 MB/s eta 0:00:02\n",
            "     ------ --------------------------------- 7.0/41.3 MB 32.2 MB/s eta 0:00:02\n",
            "     -------- ------------------------------- 8.9/41.3 MB 33.4 MB/s eta 0:00:01\n",
            "     --------- ----------------------------- 10.6/41.3 MB 34.4 MB/s eta 0:00:01\n",
            "     ----------- --------------------------- 12.5/41.3 MB 34.4 MB/s eta 0:00:01\n",
            "     ------------- ------------------------- 14.3/41.3 MB 38.5 MB/s eta 0:00:01\n",
            "     -------------- ------------------------ 15.7/41.3 MB 34.4 MB/s eta 0:00:01\n",
            "     ---------------- ---------------------- 17.2/41.3 MB 34.4 MB/s eta 0:00:01\n",
            "     ----------------- --------------------- 18.3/41.3 MB 32.8 MB/s eta 0:00:01\n",
            "     ------------------- ------------------- 20.3/41.3 MB 34.4 MB/s eta 0:00:01\n",
            "     -------------------- ------------------ 21.8/41.3 MB 32.7 MB/s eta 0:00:01\n",
            "     --------------------- ----------------- 23.2/41.3 MB 31.2 MB/s eta 0:00:01\n",
            "     ----------------------- --------------- 24.4/41.3 MB 29.7 MB/s eta 0:00:01\n",
            "     ------------------------ -------------- 26.1/41.3 MB 31.2 MB/s eta 0:00:01\n",
            "     -------------------------- ------------ 27.6/41.3 MB 31.2 MB/s eta 0:00:01\n",
            "     --------------------------- ----------- 29.3/41.3 MB 32.8 MB/s eta 0:00:01\n",
            "     ----------------------------- --------- 30.7/41.3 MB 29.7 MB/s eta 0:00:01\n",
            "     ------------------------------ -------- 32.6/41.3 MB 29.7 MB/s eta 0:00:01\n",
            "     ------------------------------- ------- 33.4/41.3 MB 29.8 MB/s eta 0:00:01\n",
            "     -------------------------------- ------ 34.8/41.3 MB 32.7 MB/s eta 0:00:01\n",
            "     ---------------------------------- ---- 36.1/41.3 MB 31.1 MB/s eta 0:00:01\n",
            "     ----------------------------------- --- 37.3/41.3 MB 31.2 MB/s eta 0:00:01\n",
            "     ------------------------------------ -- 38.7/41.3 MB 32.7 MB/s eta 0:00:01\n",
            "     --------------------------------------  40.3/41.3 MB 32.7 MB/s eta 0:00:01\n",
            "     --------------------------------------  41.3/41.3 MB 32.7 MB/s eta 0:00:01\n",
            "     --------------------------------------  41.3/41.3 MB 32.7 MB/s eta 0:00:01\n",
            "     --------------------------------------- 41.3/41.3 MB 23.3 MB/s eta 0:00:00\n",
            "Collecting scikit-image>=0.16.1\n",
            "  Downloading scikit_image-0.25.2-cp310-cp310-win_amd64.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     --- ------------------------------------ 1.3/12.8 MB 40.3 MB/s eta 0:00:01\n",
            "     -------- ------------------------------- 2.9/12.8 MB 45.3 MB/s eta 0:00:01\n",
            "     ------------- -------------------------- 4.2/12.8 MB 33.6 MB/s eta 0:00:01\n",
            "     ------------------ --------------------- 5.8/12.8 MB 33.9 MB/s eta 0:00:01\n",
            "     ---------------------- ----------------- 7.3/12.8 MB 33.4 MB/s eta 0:00:01\n",
            "     ---------------------------- ----------- 9.1/12.8 MB 34.3 MB/s eta 0:00:01\n",
            "     ------------------------------- ------- 10.4/12.8 MB 31.2 MB/s eta 0:00:01\n",
            "     ----------------------------------- --- 11.7/12.8 MB 31.2 MB/s eta 0:00:01\n",
            "     --------------------------------------  12.8/12.8 MB 29.8 MB/s eta 0:00:01\n",
            "     --------------------------------------- 12.8/12.8 MB 26.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.11.1 in c:\\users\\shreya suresh\\projects\\generic\\venv\\lib\\site-packages (from albumentations==1.1.0) (2.1.2)\n",
            "Collecting qudida>=0.0.4\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: torch in c:\\users\\shreya suresh\\projects\\generic\\venv\\lib\\site-packages (from efficientnet_pytorch) (2.5.1+cpu)\n",
            "Collecting protobuf<5,>=4.25.3\n",
            "  Using cached protobuf-4.25.8-cp310-abi3-win_amd64.whl (413 kB)\n",
            "Collecting jax\n",
            "  Downloading jax-0.6.2-py3-none-any.whl (2.7 MB)\n",
            "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
            "     --------------- ------------------------ 1.0/2.7 MB 22.0 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 2.3/2.7 MB 25.0 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 2.7/2.7 MB 24.9 MB/s eta 0:00:00\n",
            "Collecting numpy>=1.11.1\n",
            "  Downloading numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
            "     ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
            "     --- ------------------------------------ 1.6/15.8 MB 33.1 MB/s eta 0:00:01\n",
            "     ------- -------------------------------- 3.1/15.8 MB 39.7 MB/s eta 0:00:01\n",
            "     ------------ --------------------------- 4.8/15.8 MB 34.3 MB/s eta 0:00:01\n",
            "     -------------- ------------------------- 5.7/15.8 MB 27.8 MB/s eta 0:00:01\n",
            "     ------------------ --------------------- 7.3/15.8 MB 29.2 MB/s eta 0:00:01\n",
            "     --------------------- ------------------ 8.6/15.8 MB 30.6 MB/s eta 0:00:01\n",
            "     ------------------------ --------------- 9.6/15.8 MB 29.2 MB/s eta 0:00:01\n",
            "     -------------------------- ------------ 10.7/15.8 MB 27.3 MB/s eta 0:00:01\n",
            "     ----------------------------- --------- 11.9/15.8 MB 26.2 MB/s eta 0:00:01\n",
            "     --------------------------------- ----- 13.6/15.8 MB 27.3 MB/s eta 0:00:01\n",
            "     ------------------------------------ -- 14.9/15.8 MB 27.3 MB/s eta 0:00:01\n",
            "     --------------------------------------  15.8/15.8 MB 27.3 MB/s eta 0:00:01\n",
            "     --------------------------------------- 15.8/15.8 MB 25.2 MB/s eta 0:00:00\n",
            "Collecting attrs>=19.1.0\n",
            "  Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
            "     ---------------------------------------- 0.0/67.6 kB ? eta -:--:--\n",
            "     ---------------------------------------- 67.6/67.6 kB ? eta 0:00:00\n",
            "Collecting absl-py\n",
            "  Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
            "Collecting sounddevice>=0.4.4\n",
            "  Downloading sounddevice-0.5.3-py3-none-win_amd64.whl (364 kB)\n",
            "     ---------------------------------------- 0.0/364.0 kB ? eta -:--:--\n",
            "     ------------------------------------- 364.0/364.0 kB 11.4 MB/s eta 0:00:00\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.7-cp310-cp310-win_amd64.whl (8.1 MB)\n",
            "     ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
            "     ------ --------------------------------- 1.4/8.1 MB 42.6 MB/s eta 0:00:01\n",
            "     --------- ------------------------------ 2.0/8.1 MB 31.5 MB/s eta 0:00:01\n",
            "     ---------------- ----------------------- 3.3/8.1 MB 29.7 MB/s eta 0:00:01\n",
            "     ----------------------- ---------------- 4.7/8.1 MB 27.3 MB/s eta 0:00:01\n",
            "     ------------------------------- -------- 6.3/8.1 MB 28.9 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 7.8/8.1 MB 27.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 8.1/8.1 MB 25.9 MB/s eta 0:00:00\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.6.2-cp310-cp310-win_amd64.whl (57.9 MB)\n",
            "     ---------------------------------------- 0.0/57.9 MB ? eta -:--:--\n",
            "      --------------------------------------- 1.2/57.9 MB 18.4 MB/s eta 0:00:04\n",
            "     -- ------------------------------------- 3.0/57.9 MB 31.7 MB/s eta 0:00:02\n",
            "     --- ------------------------------------ 4.7/57.9 MB 33.3 MB/s eta 0:00:02\n",
            "     ---- ----------------------------------- 6.3/57.9 MB 33.7 MB/s eta 0:00:02\n",
            "     ----- ---------------------------------- 7.4/57.9 MB 31.6 MB/s eta 0:00:02\n",
            "     ------ --------------------------------- 9.3/57.9 MB 29.7 MB/s eta 0:00:02\n",
            "     ------- ------------------------------- 10.7/57.9 MB 32.7 MB/s eta 0:00:02\n",
            "     -------- ------------------------------ 12.5/57.9 MB 32.7 MB/s eta 0:00:02\n",
            "     --------- ----------------------------- 13.6/57.9 MB 32.7 MB/s eta 0:00:02\n",
            "     --------- ----------------------------- 13.6/57.9 MB 32.7 MB/s eta 0:00:02\n",
            "     --------- ----------------------------- 14.5/57.9 MB 25.2 MB/s eta 0:00:02\n",
            "     ---------- ---------------------------- 16.1/57.9 MB 25.2 MB/s eta 0:00:02\n",
            "     ----------- --------------------------- 17.2/57.9 MB 25.2 MB/s eta 0:00:02\n",
            "     ------------ -------------------------- 18.8/57.9 MB 26.2 MB/s eta 0:00:02\n",
            "     ------------- ------------------------- 20.4/57.9 MB 26.2 MB/s eta 0:00:02\n",
            "     -------------- ------------------------ 21.9/57.9 MB 24.2 MB/s eta 0:00:02\n",
            "     --------------- ----------------------- 23.4/57.9 MB 25.2 MB/s eta 0:00:02\n",
            "     ---------------- ---------------------- 24.0/57.9 MB 31.2 MB/s eta 0:00:02\n",
            "     ---------------- ---------------------- 25.1/57.9 MB 29.8 MB/s eta 0:00:02\n",
            "     ----------------- --------------------- 26.2/57.9 MB 29.7 MB/s eta 0:00:02\n",
            "     ------------------ -------------------- 27.9/57.9 MB 28.5 MB/s eta 0:00:02\n",
            "     ------------------- ------------------- 29.3/57.9 MB 28.5 MB/s eta 0:00:02\n",
            "     -------------------- ------------------ 30.6/57.9 MB 27.3 MB/s eta 0:00:01\n",
            "     --------------------- ----------------- 32.1/57.9 MB 29.7 MB/s eta 0:00:01\n",
            "     ---------------------- ---------------- 33.0/57.9 MB 29.8 MB/s eta 0:00:01\n",
            "     ----------------------- --------------- 34.5/57.9 MB 29.7 MB/s eta 0:00:01\n",
            "     ------------------------ -------------- 36.1/57.9 MB 31.2 MB/s eta 0:00:01\n",
            "     ------------------------- ------------- 37.5/57.9 MB 29.7 MB/s eta 0:00:01\n",
            "     -------------------------- ------------ 39.1/57.9 MB 32.8 MB/s eta 0:00:01\n",
            "     --------------------------- ----------- 40.8/57.9 MB 32.8 MB/s eta 0:00:01\n",
            "     ---------------------------- ---------- 42.1/57.9 MB 31.2 MB/s eta 0:00:01\n",
            "     ----------------------------- --------- 43.4/57.9 MB 29.7 MB/s eta 0:00:01\n",
            "     ------------------------------ -------- 44.8/57.9 MB 29.7 MB/s eta 0:00:01\n",
            "     ------------------------------ -------- 46.0/57.9 MB 28.5 MB/s eta 0:00:01\n",
            "     ------------------------------- ------- 47.4/57.9 MB 27.3 MB/s eta 0:00:01\n",
            "     --------------------------------- ----- 49.0/57.9 MB 28.4 MB/s eta 0:00:01\n",
            "     --------------------------------- ----- 50.4/57.9 MB 28.5 MB/s eta 0:00:01\n",
            "     ---------------------------------- ---- 51.5/57.9 MB 26.2 MB/s eta 0:00:01\n",
            "     ----------------------------------- --- 52.7/57.9 MB 25.2 MB/s eta 0:00:01\n",
            "     ------------------------------------ -- 54.7/57.9 MB 27.3 MB/s eta 0:00:01\n",
            "     ------------------------------------- - 56.3/57.9 MB 31.2 MB/s eta 0:00:01\n",
            "     --------------------------------------  57.4/57.9 MB 29.7 MB/s eta 0:00:01\n",
            "     --------------------------------------  57.9/57.9 MB 29.7 MB/s eta 0:00:01\n",
            "     --------------------------------------  57.9/57.9 MB 29.7 MB/s eta 0:00:01\n",
            "     --------------------------------------- 57.9/57.9 MB 21.1 MB/s eta 0:00:00\n",
            "Collecting opencv-contrib-python\n",
            "  Downloading opencv_contrib_python-4.12.0.88-cp37-abi3-win_amd64.whl (45.3 MB)\n",
            "     ---------------------------------------- 0.0/45.3 MB ? eta -:--:--\n",
            "     - -------------------------------------- 1.7/45.3 MB 55.2 MB/s eta 0:00:01\n",
            "     -- ------------------------------------- 2.8/45.3 MB 35.4 MB/s eta 0:00:02\n",
            "     --- ------------------------------------ 4.1/45.3 MB 29.2 MB/s eta 0:00:02\n",
            "     ---- ----------------------------------- 5.0/45.3 MB 29.1 MB/s eta 0:00:02\n",
            "     ----- ---------------------------------- 6.1/45.3 MB 25.9 MB/s eta 0:00:02\n",
            "     ----- ---------------------------------- 6.7/45.3 MB 24.0 MB/s eta 0:00:02\n",
            "     ------- -------------------------------- 8.0/45.3 MB 24.3 MB/s eta 0:00:02\n",
            "     ------- -------------------------------- 8.8/45.3 MB 24.5 MB/s eta 0:00:02\n",
            "     -------- ------------------------------- 9.8/45.3 MB 24.1 MB/s eta 0:00:02\n",
            "     --------- ----------------------------- 10.7/45.3 MB 21.8 MB/s eta 0:00:02\n",
            "     ---------- ---------------------------- 11.7/45.3 MB 21.8 MB/s eta 0:00:02\n",
            "     ---------- ---------------------------- 12.6/45.3 MB 21.1 MB/s eta 0:00:02\n",
            "     ----------- --------------------------- 13.6/45.3 MB 21.1 MB/s eta 0:00:02\n",
            "     ------------ -------------------------- 14.2/45.3 MB 20.5 MB/s eta 0:00:02\n",
            "     ------------ -------------------------- 15.0/45.3 MB 19.9 MB/s eta 0:00:02\n",
            "     ------------- ------------------------- 16.1/45.3 MB 19.9 MB/s eta 0:00:02\n",
            "     --------------- ----------------------- 17.6/45.3 MB 22.6 MB/s eta 0:00:02\n",
            "     ---------------- ---------------------- 19.3/45.3 MB 22.6 MB/s eta 0:00:02\n",
            "     ----------------- --------------------- 20.4/45.3 MB 23.4 MB/s eta 0:00:02\n",
            "     ------------------- ------------------- 22.3/45.3 MB 25.2 MB/s eta 0:00:01\n",
            "     -------------------- ------------------ 23.6/45.3 MB 25.2 MB/s eta 0:00:01\n",
            "     --------------------- ----------------- 24.9/45.3 MB 29.8 MB/s eta 0:00:01\n",
            "     ---------------------- ---------------- 26.3/45.3 MB 31.2 MB/s eta 0:00:01\n",
            "     ----------------------- --------------- 27.7/45.3 MB 29.7 MB/s eta 0:00:01\n",
            "     ------------------------ -------------- 28.5/45.3 MB 29.8 MB/s eta 0:00:01\n",
            "     ------------------------- ------------- 30.1/45.3 MB 31.2 MB/s eta 0:00:01\n",
            "     --------------------------- ----------- 31.4/45.3 MB 31.2 MB/s eta 0:00:01\n",
            "     ---------------------------- ---------- 32.9/45.3 MB 29.7 MB/s eta 0:00:01\n",
            "     ----------------------------- --------- 34.3/45.3 MB 29.7 MB/s eta 0:00:01\n",
            "     ------------------------------ -------- 35.8/45.3 MB 32.8 MB/s eta 0:00:01\n",
            "     -------------------------------- ------ 37.4/45.3 MB 31.2 MB/s eta 0:00:01\n",
            "     -------------------------------- ------ 38.3/45.3 MB 32.7 MB/s eta 0:00:01\n",
            "     ---------------------------------- ---- 40.0/45.3 MB 32.7 MB/s eta 0:00:01\n",
            "     ---------------------------------- ---- 40.4/45.3 MB 31.2 MB/s eta 0:00:01\n",
            "     ----------------------------------- --- 41.4/45.3 MB 29.7 MB/s eta 0:00:01\n",
            "     ------------------------------------ -- 42.5/45.3 MB 29.7 MB/s eta 0:00:01\n",
            "     ------------------------------------- - 43.9/45.3 MB 27.3 MB/s eta 0:00:01\n",
            "     --------------------------------------  45.3/45.3 MB 26.2 MB/s eta 0:00:01\n",
            "     --------------------------------------  45.3/45.3 MB 26.2 MB/s eta 0:00:01\n",
            "     --------------------------------------  45.3/45.3 MB 26.2 MB/s eta 0:00:01\n",
            "     --------------------------------------- 45.3/45.3 MB 18.2 MB/s eta 0:00:00\n",
            "Collecting flatbuffers>=2.0\n",
            "  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.2.1-cp310-cp310-win_amd64.whl (1.1 MB)\n",
            "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
            "     ------------------------------ --------- 0.8/1.1 MB 25.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 1.1/1.1 MB 22.2 MB/s eta 0:00:00\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-1.1.4-py3-none-any.whl (515 kB)\n",
            "     ---------------------------------------- 0.0/515.6 kB ? eta -:--:--\n",
            "     ------------------------------------- 515.6/515.6 kB 31.6 MB/s eta 0:00:00\n",
            "Collecting safetensors\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
            "     ---------------------------------------- 0.0/320.2 kB ? eta -:--:--\n",
            "     ---------------------------------------- 320.2/320.2 kB ? eta 0:00:00\n",
            "Requirement already satisfied: torchvision in c:\\users\\shreya suresh\\projects\\generic\\venv\\lib\\site-packages (from timm) (0.20.1+cpu)\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl (39.4 MB)\n",
            "     ---------------------------------------- 0.0/39.4 MB ? eta -:--:--\n",
            "     - -------------------------------------- 1.0/39.4 MB 33.4 MB/s eta 0:00:02\n",
            "     -- ------------------------------------- 2.4/39.4 MB 31.2 MB/s eta 0:00:02\n",
            "     ---- ----------------------------------- 4.1/39.4 MB 32.8 MB/s eta 0:00:02\n",
            "     ----- ---------------------------------- 5.9/39.4 MB 31.4 MB/s eta 0:00:02\n",
            "     ------- -------------------------------- 7.7/39.4 MB 35.2 MB/s eta 0:00:01\n",
            "     --------- ------------------------------ 9.3/39.4 MB 33.0 MB/s eta 0:00:01\n",
            "     ---------- ---------------------------- 11.0/39.4 MB 32.7 MB/s eta 0:00:01\n",
            "     ------------ -------------------------- 12.5/39.4 MB 34.4 MB/s eta 0:00:01\n",
            "     ------------- ------------------------- 13.5/39.4 MB 32.7 MB/s eta 0:00:01\n",
            "     -------------- ------------------------ 14.7/39.4 MB 32.7 MB/s eta 0:00:01\n",
            "     --------------- ----------------------- 15.9/39.4 MB 32.8 MB/s eta 0:00:01\n",
            "     ----------------- --------------------- 17.8/39.4 MB 32.7 MB/s eta 0:00:01\n",
            "     ------------------- ------------------- 19.3/39.4 MB 31.2 MB/s eta 0:00:01\n",
            "     --------------------- ----------------- 21.2/39.4 MB 32.8 MB/s eta 0:00:01\n",
            "     ---------------------- ---------------- 22.9/39.4 MB 32.7 MB/s eta 0:00:01\n",
            "     ------------------------ -------------- 24.7/39.4 MB 32.7 MB/s eta 0:00:01\n",
            "     ------------------------- ------------- 25.9/39.4 MB 32.7 MB/s eta 0:00:01\n",
            "     --------------------------- ----------- 27.7/39.4 MB 32.8 MB/s eta 0:00:01\n",
            "     ----------------------------- --------- 29.3/39.4 MB 34.4 MB/s eta 0:00:01\n",
            "     ------------------------------ -------- 30.8/39.4 MB 34.4 MB/s eta 0:00:01\n",
            "     ------------------------------- ------- 31.7/39.4 MB 32.8 MB/s eta 0:00:01\n",
            "     -------------------------------- ------ 33.2/39.4 MB 32.8 MB/s eta 0:00:01\n",
            "     ---------------------------------- ---- 34.4/39.4 MB 32.7 MB/s eta 0:00:01\n",
            "     ----------------------------------- --- 36.0/39.4 MB 31.2 MB/s eta 0:00:01\n",
            "     ------------------------------------- - 37.4/39.4 MB 32.8 MB/s eta 0:00:01\n",
            "     --------------------------------------  38.5/39.4 MB 29.7 MB/s eta 0:00:01\n",
            "     --------------------------------------  39.4/39.4 MB 31.2 MB/s eta 0:00:01\n",
            "     --------------------------------------  39.4/39.4 MB 31.2 MB/s eta 0:00:01\n",
            "     --------------------------------------- 39.4/39.4 MB 24.2 MB/s eta 0:00:00\n",
            "Collecting scikit-learn>=0.19.1\n",
            "  Downloading scikit_learn-1.7.2-cp310-cp310-win_amd64.whl (8.9 MB)\n",
            "     ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
            "     ----- ---------------------------------- 1.3/8.9 MB 27.7 MB/s eta 0:00:01\n",
            "     ----------- ---------------------------- 2.5/8.9 MB 32.3 MB/s eta 0:00:01\n",
            "     ------------------- -------------------- 4.3/8.9 MB 30.1 MB/s eta 0:00:01\n",
            "     -------------------------- ------------- 5.9/8.9 MB 31.3 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 7.9/8.9 MB 33.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  8.9/8.9 MB 31.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 8.9/8.9 MB 28.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\shreya suresh\\projects\\generic\\venv\\lib\\site-packages (from qudida>=0.0.4->albumentations==1.1.0) (4.9.0)\n",
            "Collecting imageio!=2.35.0,>=2.33\n",
            "  Downloading imageio-2.37.2-py3-none-any.whl (317 kB)\n",
            "     ---------------------------------------- 0.0/317.6 kB ? eta -:--:--\n",
            "     ------------------------------------- 317.6/317.6 kB 20.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: networkx>=3.0 in c:\\users\\shreya suresh\\projects\\generic\\venv\\lib\\site-packages (from scikit-image>=0.16.1->albumentations==1.1.0) (3.3)\n",
            "Requirement already satisfied: packaging>=21 in c:\\users\\shreya suresh\\projects\\generic\\venv\\lib\\site-packages (from scikit-image>=0.16.1->albumentations==1.1.0) (25.0)\n",
            "Collecting lazy-loader>=0.4\n",
            "  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: pillow>=10.1 in c:\\users\\shreya suresh\\projects\\generic\\venv\\lib\\site-packages (from scikit-image>=0.16.1->albumentations==1.1.0) (11.3.0)\n",
            "Collecting tifffile>=2022.8.12\n",
            "  Downloading tifffile-2025.5.10-py3-none-any.whl (226 kB)\n",
            "     ---------------------------------------- 0.0/226.5 kB ? eta -:--:--\n",
            "     ------------------------------------- 226.5/226.5 kB 13.5 MB/s eta 0:00:00\n",
            "Collecting CFFI>=1.0\n",
            "  Downloading cffi-2.0.0-cp310-cp310-win_amd64.whl (182 kB)\n",
            "     ---------------------------------------- 0.0/182.8 kB ? eta -:--:--\n",
            "     ------------------------------------- 182.8/182.8 kB 10.8 MB/s eta 0:00:00\n",
            "Collecting httpx<1,>=0.23.0\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "     ---------------------------------------- 0.0/73.5 kB ? eta -:--:--\n",
            "     ---------------------------------------- 73.5/73.5 kB ? eta 0:00:00\n",
            "Requirement already satisfied: filelock in c:\\users\\shreya suresh\\projects\\generic\\venv\\lib\\site-packages (from huggingface_hub->timm) (3.19.1)\n",
            "Collecting shellingham\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Collecting hf-xet<2.0.0,>=1.2.0\n",
            "  Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl (2.9 MB)\n",
            "     ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
            "     -------------- ------------------------- 1.1/2.9 MB 22.4 MB/s eta 0:00:01\n",
            "     ------------------------------- -------- 2.3/2.9 MB 24.5 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 2.9/2.9 MB 26.6 MB/s eta 0:00:00\n",
            "Collecting tqdm>=4.42.1\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "     ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
            "     ---------------------------------------- 78.5/78.5 kB ? eta 0:00:00\n",
            "Collecting typer-slim\n",
            "  Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
            "     ---------------------------------------- 0.0/47.1 kB ? eta -:--:--\n",
            "     ---------------------------------------- 47.1/47.1 kB ? eta 0:00:00\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shreya suresh\\projects\\generic\\venv\\lib\\site-packages (from huggingface_hub->timm) (2025.9.0)\n",
            "Collecting ml_dtypes>=0.5.0\n",
            "  Using cached ml_dtypes-0.5.3-cp310-cp310-win_amd64.whl (206 kB)\n",
            "Collecting opt_einsum\n",
            "  Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Collecting cycler>=0.10\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.60.1-cp310-cp310-win_amd64.whl (2.3 MB)\n",
            "     ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
            "     ------------------------- -------------- 1.5/2.3 MB 46.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 2.3/2.3 MB 24.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\shreya suresh\\projects\\generic\\venv\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Collecting contourpy>=1.0.1\n",
            "  Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
            "     ---------------------------------------- 0.0/221.2 kB ? eta -:--:--\n",
            "     ------------------------------------- 221.2/221.2 kB 14.1 MB/s eta 0:00:00\n",
            "Collecting kiwisolver>=1.3.1\n",
            "  Downloading kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)\n",
            "     ---------------------------------------- 0.0/73.7 kB ? eta -:--:--\n",
            "     ---------------------------------------- 73.7/73.7 kB 4.2 MB/s eta 0:00:00\n",
            "Collecting pyparsing>=3\n",
            "  Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "     ---------------------------------------- 0.0/113.9 kB ? eta -:--:--\n",
            "     ---------------------------------------- 113.9/113.9 kB ? eta 0:00:00\n",
            "Collecting opencv-contrib-python\n",
            "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-win_amd64.whl (46.2 MB)\n",
            "     ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
            "      --------------------------------------- 0.9/46.2 MB 14.5 MB/s eta 0:00:04\n",
            "     -- ------------------------------------- 2.6/46.2 MB 23.9 MB/s eta 0:00:02\n",
            "     --- ------------------------------------ 3.9/46.2 MB 25.2 MB/s eta 0:00:02\n",
            "     ---- ----------------------------------- 5.6/46.2 MB 29.8 MB/s eta 0:00:02\n",
            "     ------ --------------------------------- 7.2/46.2 MB 30.9 MB/s eta 0:00:02\n",
            "     ------- -------------------------------- 8.7/46.2 MB 30.8 MB/s eta 0:00:02\n",
            "     -------- ------------------------------ 10.0/46.2 MB 30.4 MB/s eta 0:00:02\n",
            "     --------- ----------------------------- 11.8/46.2 MB 32.7 MB/s eta 0:00:02\n",
            "     ----------- --------------------------- 13.3/46.2 MB 32.7 MB/s eta 0:00:02\n",
            "     ------------ -------------------------- 15.1/46.2 MB 34.6 MB/s eta 0:00:01\n",
            "     ------------- ------------------------- 16.4/46.2 MB 34.4 MB/s eta 0:00:01\n",
            "     --------------- ----------------------- 17.8/46.2 MB 31.2 MB/s eta 0:00:01\n",
            "     ---------------- ---------------------- 19.4/46.2 MB 31.2 MB/s eta 0:00:01\n",
            "     ---------------- ---------------------- 19.9/46.2 MB 29.7 MB/s eta 0:00:01\n",
            "     ----------------- --------------------- 21.1/46.2 MB 29.7 MB/s eta 0:00:01\n",
            "     ------------------ -------------------- 22.3/46.2 MB 28.5 MB/s eta 0:00:01\n",
            "     -------------------- ------------------ 23.7/46.2 MB 27.3 MB/s eta 0:00:01\n",
            "     --------------------- ----------------- 25.2/46.2 MB 25.2 MB/s eta 0:00:01\n",
            "     ---------------------- ---------------- 26.8/46.2 MB 27.3 MB/s eta 0:00:01\n",
            "     ----------------------- --------------- 28.0/46.2 MB 27.3 MB/s eta 0:00:01\n",
            "     ------------------------- ------------- 29.7/46.2 MB 27.3 MB/s eta 0:00:01\n",
            "     -------------------------- ------------ 31.1/46.2 MB 28.4 MB/s eta 0:00:01\n",
            "     --------------------------- ----------- 32.7/46.2 MB 28.5 MB/s eta 0:00:01\n",
            "     ---------------------------- ---------- 34.2/46.2 MB 29.7 MB/s eta 0:00:01\n",
            "     ------------------------------ -------- 35.6/46.2 MB 32.7 MB/s eta 0:00:01\n",
            "     ------------------------------- ------- 37.4/46.2 MB 32.8 MB/s eta 0:00:01\n",
            "     -------------------------------- ------ 38.5/46.2 MB 31.2 MB/s eta 0:00:01\n",
            "     --------------------------------- ----- 40.0/46.2 MB 31.2 MB/s eta 0:00:01\n",
            "     ----------------------------------- --- 41.5/46.2 MB 32.7 MB/s eta 0:00:01\n",
            "     ------------------------------------ -- 43.4/46.2 MB 32.7 MB/s eta 0:00:01\n",
            "     ------------------------------------- - 44.9/46.2 MB 31.2 MB/s eta 0:00:01\n",
            "     --------------------------------------  46.2/46.2 MB 29.7 MB/s eta 0:00:01\n",
            "     --------------------------------------  46.2/46.2 MB 29.7 MB/s eta 0:00:01\n",
            "     --------------------------------------  46.2/46.2 MB 29.7 MB/s eta 0:00:01\n",
            "     --------------------------------------- 46.2/46.2 MB 21.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\shreya suresh\\projects\\generic\\venv\\lib\\site-packages (from torch->efficientnet_pytorch) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\shreya suresh\\projects\\generic\\venv\\lib\\site-packages (from torch->efficientnet_pytorch) (3.1.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shreya suresh\\projects\\generic\\venv\\lib\\site-packages (from sympy==1.13.1->torch->efficientnet_pytorch) (1.3.0)\n",
            "Collecting pycparser\n",
            "  Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
            "     ---------------------------------------- 0.0/118.1 kB ? eta -:--:--\n",
            "     -------------------------------------- 118.1/118.1 kB 6.7 MB/s eta 0:00:00\n",
            "Collecting idna\n",
            "  Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
            "Collecting httpcore==1.*\n",
            "  Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "     ---------------------------------------- 0.0/78.8 kB ? eta -:--:--\n",
            "     ---------------------------------------- 78.8/78.8 kB ? eta 0:00:00\n",
            "Collecting anyio\n",
            "  Downloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
            "     ---------------------------------------- 0.0/109.1 kB ? eta -:--:--\n",
            "     ---------------------------------------- 109.1/109.1 kB ? eta 0:00:00\n",
            "Collecting certifi\n",
            "  Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
            "     ---------------------------------------- 0.0/159.4 kB ? eta -:--:--\n",
            "     -------------------------------------- 159.4/159.4 kB 9.9 MB/s eta 0:00:00\n",
            "Collecting h11>=0.16\n",
            "  Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\shreya suresh\\projects\\generic\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Collecting threadpoolctl>=3.1.0\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Collecting joblib>=1.2.0\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "     ---------------------------------------- 0.0/308.4 kB ? eta -:--:--\n",
            "     ------------------------------------- 308.4/308.4 kB 19.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: colorama in c:\\users\\shreya suresh\\projects\\generic\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shreya suresh\\projects\\generic\\venv\\lib\\site-packages (from jinja2->torch->efficientnet_pytorch) (2.1.5)\n",
            "Collecting click>=8.0.0\n",
            "  Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
            "     ---------------------------------------- 0.0/107.3 kB ? eta -:--:--\n",
            "     -------------------------------------- 107.3/107.3 kB 3.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\shreya suresh\\projects\\generic\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub->timm) (1.3.0)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: flatbuffers, tqdm, threadpoolctl, sniffio, shellingham, sentencepiece, safetensors, PyYAML, pyparsing, pycparser, protobuf, opt_einsum, numpy, lazy-loader, kiwisolver, joblib, idna, hf-xet, h11, fonttools, cycler, click, certifi, attrs, absl-py, typer-slim, tifffile, scipy, opencv-python-headless, opencv-contrib-python, ml_dtypes, imageio, httpcore, contourpy, CFFI, anyio, sounddevice, scikit-learn, scikit-image, matplotlib, jaxlib, httpx, efficientnet_pytorch, qudida, jax, huggingface_hub, timm, mediapipe, albumentations\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.2\n",
            "    Uninstalling numpy-2.1.2:\n",
            "      Successfully uninstalled numpy-2.1.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Shreya Suresh\\\\projects\\\\generic\\\\venv\\\\Lib\\\\site-packages\\\\~umpy.libs\\\\libscipy_openblas64_-c16e4918366c6bc1f1cd71e28ca36fc0.dll'\n",
            "Check the permissions.\n",
            "\n",
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to install required libraries\n",
        "# Note: This may take 2-5 minutes, especially mediapipe which is a large package\n",
        "# You only need to run this once. If packages are already installed, it will skip them.\n",
        "\n",
        "# Core ML packages\n",
        "%pip install torch torchvision torchaudio\n",
        "\n",
        "# Model and augmentation libraries\n",
        "%pip install efficientnet_pytorch mediapipe albumentations==1.1.0 timm\n",
        "\n",
        "# Data science and visualization\n",
        "%pip install numpy matplotlib pandas seaborn scikit-learn\n",
        "\n",
        "# Utilities\n",
        "%pip install opencv-python tqdm ipywidgets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_uh3J2AJlyD",
        "outputId": "bec46c9e-f4bc-440b-ea54-6acba991263e"
      },
      "outputs": [],
      "source": [
        "# Dataset and checkpoint paths (relative to notebook location)\n",
        "DATA_ROOT = \"./dataset\"  # folder that contains Cataract/Conjunctivitis/Normal\n",
        "CHECKPOINT_DIR = \"./checkpoints\"\n",
        "\n",
        "# Create checkpoint directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "phE4aPdBJozW"
      },
      "outputs": [],
      "source": [
        "import os, random, time, copy\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXyV61MaI1rQ",
        "outputId": "c8f44dd2-c229-4cc4-8e77-e216ec9b63bf"
      },
      "outputs": [],
      "source": [
        "# This cell is not needed - all packages are installed in Cell 0\n",
        "# Removed duplicate installation to speed up execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxMLdPV4JSOl",
        "outputId": "cdf88798-b31b-406e-983e-524a50c88186"
      },
      "outputs": [],
      "source": [
        "# This cell is not needed - all packages are installed in Cell 0\n",
        "# Removed duplicate installation to speed up execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwtKCO9jLYgp",
        "outputId": "429c143e-278f-4a6d-cb1d-d1af3c0206ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Shreya Suresh\\projects\\generic\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from efficientnet_pytorch import EfficientNet\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import timm  # optional if you want timm-pretrained models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxNrKYHwKvZ4",
        "outputId": "5c3c6699-9a38-4dd8-971e-91932df5cae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KrS0r-EoKUF-"
      },
      "outputs": [],
      "source": [
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "def crop_eye_region_bgr(image_bgr):\n",
        "    \"\"\"\n",
        "    Input: BGR numpy image (OpenCV)\n",
        "    Output: cropped BGR image (fallback to original if no face/eye found)\n",
        "    \"\"\"\n",
        "    h, w = image_bgr.shape[:2]\n",
        "    img_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5) as fm:\n",
        "        res = fm.process(img_rgb)\n",
        "        if not res.multi_face_landmarks:\n",
        "            return image_bgr\n",
        "        lm = res.multi_face_landmarks[0].landmark\n",
        "        LEFT_EYE_IDX = [33, 133, 159, 145]\n",
        "        RIGHT_EYE_IDX = [263, 362, 386, 374]\n",
        "        def get_box(indices):\n",
        "            xs = [int(lm[i].x * w) for i in indices]\n",
        "            ys = [int(lm[i].y * h) for i in indices]\n",
        "            x0, x1 = max(0,min(xs)), min(w, max(xs))\n",
        "            y0, y1 = max(0,min(ys)), min(h, max(ys))\n",
        "            pad = int(0.35 * max(y1-y0, x1-x0))  # relative padding\n",
        "            return max(0,x0-pad), max(0,y0-pad), min(w,x1+pad), min(h,y1+pad)\n",
        "        lx0, ly0, lx1, ly1 = get_box(LEFT_EYE_IDX)\n",
        "        rx0, ry0, rx1, ry1 = get_box(RIGHT_EYE_IDX)\n",
        "        left = image_bgr[ly0:ly1, lx0:lx1]\n",
        "        right = image_bgr[ry0:ry1, rx0:rx1]\n",
        "        # choose the larger crop (better detail)\n",
        "        if left.size==0 and right.size==0:\n",
        "            return image_bgr\n",
        "        if left.size >= right.size:\n",
        "            return left if left.size>0 else right\n",
        "        else:\n",
        "            return right\n",
        "\n",
        "def apply_clahe_rgb(image_bgr):\n",
        "    \"\"\"\n",
        "    Apply CLAHE in LAB space to preserve colors.\n",
        "    Input/Output: BGR numpy image\n",
        "    \"\"\"\n",
        "    lab = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    cl = clahe.apply(l)\n",
        "    limg = cv2.merge((cl,a,b))\n",
        "    final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
        "    return final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNXV6kKrM60n",
        "outputId": "ef790f4f-7034-41a5-c4d2-963f2f81ad80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes mapping: {'Cataract': 0, 'Conjunctivitis': 1, 'Normal': 2}\n",
            "Train samples: 1240 Val samples: 310\n"
          ]
        }
      ],
      "source": [
        "CLASS_FILTER = [\"Cataract\", \"Conjunctivitis\", \"Normal\"]  # keep only these\n",
        "\n",
        "# Albumentations transforms\n",
        "train_aug = A.Compose([\n",
        "    A.RandomResizedCrop(300,300, scale=(0.8,1.0), ratio=(0.9,1.1)),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Rotate(limit=15, p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.5),\n",
        "    A.HueSaturationValue(p=0.3),\n",
        "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.4),\n",
        "    A.CoarseDropout(max_holes=1, max_height=30, max_width=30, p=0.2),\n",
        "    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "val_aug = A.Compose([\n",
        "    A.Resize(300,300),\n",
        "    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "class EyeDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', filter_classes=CLASS_FILTER, transform=None):\n",
        "        self.samples = []\n",
        "        self.class_to_idx = {}\n",
        "        classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir,d))])\n",
        "        idx = 0\n",
        "        for c in classes:\n",
        "            if c in filter_classes:\n",
        "                self.class_to_idx[c] = idx\n",
        "                for f in os.listdir(os.path.join(root_dir,c)):\n",
        "                    if f.lower().endswith(('.jpg','.jpeg','.png')):\n",
        "                        self.samples.append((os.path.join(root_dir,c,f), idx))\n",
        "                idx += 1\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        path, label = self.samples[i]\n",
        "        img_bgr = cv2.imread(path)\n",
        "        if img_bgr is None:\n",
        "            # fallback blank image\n",
        "            img_bgr = np.zeros((400,400,3), dtype=np.uint8)\n",
        "        # Crop eye region\n",
        "        cropped = crop_eye_region_bgr(img_bgr)\n",
        "        # Apply CLAHE\n",
        "        enhanced = apply_clahe_rgb(cropped)\n",
        "        # Convert BGR->RGB for albumentations\n",
        "        img_rgb = cv2.cvtColor(enhanced, cv2.COLOR_BGR2RGB)\n",
        "        if self.transform:\n",
        "            data = self.transform(image=img_rgb)\n",
        "            img_tensor = data['image']\n",
        "        else:\n",
        "            # basic conversion\n",
        "            img_tensor = val_aug(image=img_rgb)['image']\n",
        "        return img_tensor, label\n",
        "\n",
        "# Create datasets\n",
        "full_dataset = EyeDataset(DATA_ROOT, split='all', transform=None)\n",
        "# We'll do an 80-20 split\n",
        "indices = list(range(len(full_dataset)))\n",
        "random.seed(42)\n",
        "random.shuffle(indices)\n",
        "split = int(0.2 * len(indices))\n",
        "val_idx = indices[:split]\n",
        "train_idx = indices[split:]\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "# Create train dataset with augmentation\n",
        "train_dataset_base = Subset(full_dataset, train_idx)\n",
        "train_dataset = copy.deepcopy(train_dataset_base)\n",
        "train_dataset.dataset.transform = train_aug\n",
        "\n",
        "# Create validation dataset with validation transforms\n",
        "val_dataset_base = Subset(full_dataset, val_idx)\n",
        "val_dataset = copy.deepcopy(val_dataset_base)\n",
        "val_dataset.dataset.transform = val_aug\n",
        "\n",
        "print(\"Classes mapping:\", full_dataset.class_to_idx)\n",
        "print(\"Train samples:\", len(train_dataset), \"Val samples:\", len(val_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmjiN0hOhc4Y",
        "outputId": "32e007a3-7b79-4296-86cd-65705b6ad3c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weighted sampler created!\n",
            "Class counts: [443 283 514]\n",
            "Class weights: [0.00225734 0.00353357 0.00194553]\n"
          ]
        }
      ],
      "source": [
        "# -------------------------\n",
        "# TRAIN AUGMENTATIONS\n",
        "# ------------------------\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "# Extract training labels correctly\n",
        "train_labels_np = np.array([full_dataset.samples[i][1] for i in train_idx])\n",
        "\n",
        "# Compute class counts\n",
        "class_counts = np.bincount(train_labels_np)\n",
        "class_weights = 1.0 / class_counts\n",
        "\n",
        "# Assign weight to each sample\n",
        "weights = class_weights[train_labels_np]\n",
        "\n",
        "# Create sampler\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=weights,\n",
        "    num_samples=len(weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "print(\"Weighted sampler created!\")\n",
        "print(\"Class counts:\", class_counts)\n",
        "print(\"Class weights:\", class_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "AzrnXk-DNDy2"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhxVFBJGNL83",
        "outputId": "a4ccf965-ba15-4b05-b40f-634754002213"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b3-5fb5a3c3.pth\" to C:\\Users\\Shreya Suresh/.cache\\torch\\hub\\checkpoints\\efficientnet-b3-5fb5a3c3.pth\n",
            "100%|| 47.1M/47.1M [00:03<00:00, 13.2MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b3\n"
          ]
        }
      ],
      "source": [
        "num_classes = len(CLASS_FILTER)\n",
        "model = EfficientNet.from_pretrained('efficientnet-b3')  # loads pretrained\n",
        "# replace fc\n",
        "in_f = model._fc.in_features\n",
        "model._fc = nn.Sequential(\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(in_f, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(512, num_classes)\n",
        ")\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5vR7S8KZN6aJ"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=4, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best = None\n",
        "        self.counter = 0\n",
        "    def step(self, value):\n",
        "        if self.best is None or value < self.best - self.min_delta:\n",
        "            self.best = value\n",
        "            self.counter = 0\n",
        "            return False\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            return self.counter >= self.patience\n",
        "\n",
        "early_stopper = EarlyStopping(patience=6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYEoAZikN4wO",
        "outputId": "d0c22acf-3c5d-4d8d-9bb9-beda0550e8d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Shreya Suresh\\AppData\\Local\\Temp\\ipykernel_39140\\3885266469.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "c:\\Users\\Shreya Suresh\\projects\\generic\\venv\\lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
        "\n",
        "import time, os\n",
        "import copy\n",
        "import torch\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm   #  added for progress bar\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "# ------------------------------\n",
        "# TRAIN ONE EPOCH (FAST + PROGRESS BAR)\n",
        "# ------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    preds, trues = [], []\n",
        "\n",
        "    progress = tqdm(loader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for images, labels in progress:\n",
        "        images = images.to(device); labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():   # FP16 training\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        preds.append(torch.argmax(outputs, dim=1).detach().cpu())\n",
        "        trues.append(labels.detach().cpu())\n",
        "\n",
        "        progress.set_postfix(loss=loss.item())  # live loss update\n",
        "\n",
        "    preds = torch.cat(preds).numpy()\n",
        "    trues = torch.cat(trues).numpy()\n",
        "\n",
        "    acc = accuracy_score(trues, preds)\n",
        "    f1  = f1_score(trues, preds, average='weighted')\n",
        "\n",
        "    return running_loss / len(loader.dataset), acc, f1\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# VALIDATION (PROGRESS BAR)\n",
        "# ------------------------------\n",
        "def validate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    preds, trues = [], []\n",
        "\n",
        "    progress = tqdm(loader, desc=\"Validating\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in progress:\n",
        "            images = images.to(device); labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            preds.append(torch.argmax(outputs, dim=1).detach().cpu())\n",
        "            trues.append(labels.detach().cpu())\n",
        "\n",
        "            progress.set_postfix(loss=loss.item())  # live loss update\n",
        "\n",
        "    preds = torch.cat(preds).numpy()\n",
        "    trues = torch.cat(trues).numpy()\n",
        "\n",
        "    acc = accuracy_score(trues, preds)\n",
        "    f1  = f1_score(trues, preds, average='weighted')\n",
        "\n",
        "    return running_loss / len(loader.dataset), acc, f1, trues, preds\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7xpO9B6P1kn",
        "outputId": "6bc3b1be-c34e-44e7-d226-39b86cd42873"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Training...\n",
            "\n",
            "\n",
            "===== Epoch 1/25 (4.0%) =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/78 [00:00<?, ?it/s]C:\\Users\\Shreya Suresh\\AppData\\Local\\Temp\\ipykernel_39140\\3885266469.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():   # FP16 training\n",
            "c:\\Users\\Shreya Suresh\\projects\\generic\\venv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25 | Train Loss: 0.5059, Acc: 0.8056, F1: 0.8040 | Val Loss: 0.2258, Acc: 0.9226, F1: 0.9220 | Time: 616.6s\n",
            " Saved Best Model\n",
            "\n",
            "===== Epoch 2/25 (8.0%) =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/78 [00:00<?, ?it/s]C:\\Users\\Shreya Suresh\\AppData\\Local\\Temp\\ipykernel_39140\\3885266469.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():   # FP16 training\n",
            "c:\\Users\\Shreya Suresh\\projects\\generic\\venv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/25 | Train Loss: 0.2011, Acc: 0.9250, F1: 0.9246 | Val Loss: 0.1574, Acc: 0.9548, F1: 0.9542 | Time: 681.5s\n",
            " Saved Best Model\n",
            "\n",
            "===== Epoch 3/25 (12.0%) =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/78 [00:00<?, ?it/s]C:\\Users\\Shreya Suresh\\AppData\\Local\\Temp\\ipykernel_39140\\3885266469.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():   # FP16 training\n",
            "c:\\Users\\Shreya Suresh\\projects\\generic\\venv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/25 | Train Loss: 0.1813, Acc: 0.9282, F1: 0.9280 | Val Loss: 0.1140, Acc: 0.9645, F1: 0.9646 | Time: 581.7s\n",
            " Saved Best Model\n",
            "\n",
            "===== Epoch 4/25 (16.0%) =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/78 [00:00<?, ?it/s]C:\\Users\\Shreya Suresh\\AppData\\Local\\Temp\\ipykernel_39140\\3885266469.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():   # FP16 training\n",
            "c:\\Users\\Shreya Suresh\\projects\\generic\\venv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/25 | Train Loss: 0.1360, Acc: 0.9508, F1: 0.9507 | Val Loss: 0.1267, Acc: 0.9710, F1: 0.9707 | Time: 575.6s\n",
            "\n",
            "===== Epoch 5/25 (20.0%) =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/78 [00:00<?, ?it/s]C:\\Users\\Shreya Suresh\\AppData\\Local\\Temp\\ipykernel_39140\\3885266469.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():   # FP16 training\n",
            "c:\\Users\\Shreya Suresh\\projects\\generic\\venv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/25 | Train Loss: 0.1259, Acc: 0.9581, F1: 0.9581 | Val Loss: 0.1912, Acc: 0.9613, F1: 0.9610 | Time: 1489.1s\n",
            "\n",
            "===== Epoch 6/25 (24.0%) =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/78 [00:00<?, ?it/s]C:\\Users\\Shreya Suresh\\AppData\\Local\\Temp\\ipykernel_39140\\3885266469.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():   # FP16 training\n",
            "c:\\Users\\Shreya Suresh\\projects\\generic\\venv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/25 | Train Loss: 0.0987, Acc: 0.9645, F1: 0.9644 | Val Loss: 0.1318, Acc: 0.9710, F1: 0.9709 | Time: 591.4s\n",
            "\n",
            "===== Epoch 7/25 (28.0%) =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/78 [00:00<?, ?it/s]C:\\Users\\Shreya Suresh\\AppData\\Local\\Temp\\ipykernel_39140\\3885266469.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():   # FP16 training\n",
            "c:\\Users\\Shreya Suresh\\projects\\generic\\venv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/25 | Train Loss: 0.0891, Acc: 0.9653, F1: 0.9653 | Val Loss: 0.2333, Acc: 0.9323, F1: 0.9325 | Time: 563.8s\n",
            "\n",
            "===== Epoch 8/25 (32.0%) =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/78 [00:00<?, ?it/s]C:\\Users\\Shreya Suresh\\AppData\\Local\\Temp\\ipykernel_39140\\3885266469.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():   # FP16 training\n",
            "c:\\Users\\Shreya Suresh\\projects\\generic\\venv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "                                                                      \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[28], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_percent\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%) =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m train_loss, train_acc, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m val_loss, val_acc, val_f1, y_true, y_pred \u001b[38;5;241m=\u001b[39m validate(\n\u001b[0;32m     22\u001b[0m     model, val_loader, criterion\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     25\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(val_loss)\n",
            "Cell \u001b[1;32mIn[27], line 30\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     27\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     28\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 30\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[0;32m     32\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
            "File \u001b[1;32mc:\\Users\\Shreya Suresh\\projects\\generic\\venv\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Shreya Suresh\\projects\\generic\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Shreya Suresh\\projects\\generic\\venv\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "File \u001b[1;32mc:\\Users\\Shreya Suresh\\projects\\generic\\venv\\lib\\site-packages\\torch\\autograd\\function.py:292\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m    288\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 292\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    293\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ------------------------------\n",
        "# TRAINING LOOP\n",
        "# ------------------------------\n",
        "best_val_loss = float('inf')\n",
        "best_state = None\n",
        "EPOCHS = 25\n",
        "\n",
        "print(\"Starting Training...\\n\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "    epoch_percent = (epoch / EPOCHS) * 100\n",
        "    t0 = time.time()\n",
        "\n",
        "    print(f\"\\n===== Epoch {epoch}/{EPOCHS} ({epoch_percent:.1f}%) =====\")\n",
        "\n",
        "    train_loss, train_acc, train_f1 = train_one_epoch(\n",
        "        model, train_loader, optimizer, criterion\n",
        "    )\n",
        "\n",
        "    val_loss, val_acc, val_f1, y_true, y_pred = validate(\n",
        "        model, val_loader, criterion\n",
        "    )\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch}/{EPOCHS} | \"\n",
        "        f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f} | \"\n",
        "        f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f} | \"\n",
        "        f\"Time: {time.time()-t0:.1f}s\"\n",
        "    )\n",
        "\n",
        "    # Save best\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_state = copy.deepcopy(model.state_dict())\n",
        "        torch.save(\n",
        "            {'state_dict': best_state, 'classes': CLASS_FILTER},\n",
        "            os.path.join(CHECKPOINT_DIR, \"best_effnet_b3.pth\")\n",
        "        )\n",
        "        print(\" Saved Best Model\")\n",
        "\n",
        "    if early_stopper.step(val_loss):\n",
        "        print(\"\\n Early Stopping Triggered  Stopping Training\")\n",
        "        break\n",
        "\n",
        "\n",
        "print(\"\\nLoading Best Model...\")\n",
        "checkpoint = torch.load(os.path.join(CHECKPOINT_DIR, \"best_effnet_b3.pth\"), map_location=device)\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "print(\" Best Model Loaded Successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "-AOasQFTaHRo",
        "outputId": "f823767f-37bf-4486-c037-02d9ad879214"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "      Cataract       0.88      0.96      0.92       101\n",
            "Conjunctivitis       0.90      0.96      0.93        74\n",
            "        Normal       0.98      0.88      0.93       135\n",
            "\n",
            "      accuracy                           0.93       310\n",
            "     macro avg       0.92      0.93      0.93       310\n",
            "  weighted avg       0.93      0.93      0.93       310\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        all_preds.extend(torch.argmax(outputs,1).cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(all_labels, all_preds, target_names=CLASS_FILTER))\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(6,5)); import seaborn as sns\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=CLASS_FILTER, yticklabels=CLASS_FILTER); plt.title(\"Confusion matrix\"); plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwIDSEXZkGAG",
        "outputId": "d9d5bfe4-e306-4e94-f0e9-b317c09fa79e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Model saved successfully at: ./efficientnet_eye_model.pth\n"
          ]
        }
      ],
      "source": [
        "SAVE_PATH = \"./efficientnet_eye_model.pth\"\n",
        "\n",
        "torch.save({\n",
        "    \"state_dict\": model.state_dict(),\n",
        "    \"class_names\": CLASS_FILTER\n",
        "}, SAVE_PATH)\n",
        "\n",
        "print(f\"\\n Model saved successfully at: {SAVE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7DSFfuckJUl",
        "outputId": "6debb150-912f-4a87-8707-86e63a24259e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Model loaded successfully and ready for inference!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Shreya Suresh\\AppData\\Local\\Temp\\ipykernel_39140\\514991573.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(SAVE_PATH, map_location=device)\n"
          ]
        }
      ],
      "source": [
        "checkpoint = torch.load(SAVE_PATH, map_location=device)\n",
        "model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "CLASS_FILTER = checkpoint[\"class_names\"]\n",
        "model.eval()\n",
        "\n",
        "print(\" Model loaded successfully and ready for inference!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "KUuUTUEzawfi"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# simple TTA transforms: original, hflip, small rotate\n",
        "tta_transforms = [\n",
        "    A.Compose([A.Resize(300,300), A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)), ToTensorV2()]),\n",
        "    A.Compose([A.HorizontalFlip(p=1.0), A.Resize(300,300), A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)), ToTensorV2()]),\n",
        "    A.Compose([A.Rotate(limit=10, p=1.0), A.Resize(300,300), A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)), ToTensorV2()])\n",
        "]\n",
        "\n",
        "def preprocess_for_tta(img_path):\n",
        "    # read -> crop -> clahe -> rgb\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    cropped = crop_eye_region_bgr(img_bgr)\n",
        "    enhanced = apply_clahe_rgb(cropped)\n",
        "    img_rgb = cv2.cvtColor(enhanced, cv2.COLOR_BGR2RGB)\n",
        "    return img_rgb\n",
        "\n",
        "def predict_tta(img_path, model, tta_list=tta_transforms):\n",
        "    img_rgb = preprocess_for_tta(img_path)\n",
        "    probs_accum = np.zeros(len(CLASS_FILTER), dtype=np.float32)\n",
        "    with torch.no_grad():\n",
        "        for t in tta_list:\n",
        "            inp = t(image=img_rgb)['image'].unsqueeze(0).to(device)\n",
        "            out = model(inp)\n",
        "            probs = F.softmax(out, dim=1).cpu().numpy()[0]\n",
        "            probs_accum += probs\n",
        "    probs_accum /= len(tta_list)\n",
        "    pred_idx = int(np.argmax(probs_accum))\n",
        "    return CLASS_FILTER[pred_idx], float(probs_accum[pred_idx]), probs_accum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "rdDJVYVlXDA_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "        # Save activation maps during forward pass\n",
        "        self.forward_hook = target_layer.register_forward_hook(self._save_activations)\n",
        "\n",
        "        # Save gradients from backward pass\n",
        "        self.backward_hook = target_layer.register_backward_hook(self._save_gradients)\n",
        "\n",
        "    def _save_activations(self, module, input, output):\n",
        "        self.activations = output.detach()\n",
        "\n",
        "    def _save_gradients(self, module, grad_input, grad_output):\n",
        "        self.gradients = grad_output[0].detach()\n",
        "\n",
        "    def __call__(self, image_tensor, class_idx=None):\n",
        "        \"\"\"\n",
        "        image_tensor = (1, 3, H, W)\n",
        "        class_idx = optional target class\n",
        "        \"\"\"\n",
        "        self.model.zero_grad()\n",
        "        output = self.model(image_tensor)\n",
        "\n",
        "        # If not specified, take predicted class\n",
        "        if class_idx is None:\n",
        "            class_idx = torch.argmax(output).item()\n",
        "\n",
        "        target_score = output[0, class_idx]\n",
        "        target_score.backward()\n",
        "\n",
        "        # 1) grad-CAM weights = mean gradients across spatial dims\n",
        "        weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n",
        "\n",
        "        # 2) Weighted sum of activations\n",
        "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
        "\n",
        "        # 3) ReLU\n",
        "        cam = F.relu(cam)\n",
        "\n",
        "        # Normalize to [0,1]\n",
        "        cam = cam.squeeze().cpu().numpy()\n",
        "        cam -= cam.min()\n",
        "        cam /= cam.max() + 1e-8\n",
        "\n",
        "        return cam\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "ukGjrzRqa8Lk",
        "outputId": "3ab0cca5-8923-40b3-9c74-e854e664e656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grad-CAM ready.\n"
          ]
        }
      ],
      "source": [
        "# Attach Grad-CAM to last conv block of EfficientNet-B3\n",
        "target_layer = model._blocks[-1]._project_conv\n",
        "cam = GradCAM(model, target_layer)\n",
        "\n",
        "print(\"Grad-CAM ready.\")\n",
        "\n",
        "# Upload images from device\n",
        "from ipywidgets import FileUpload, Button\n",
        "from IPython.display import display\n",
        "\n",
        "upload_dir = \"./uploaded_images\"\n",
        "os.makedirs(upload_dir, exist_ok=True)\n",
        "test_images = []\n",
        "\n",
        "def process_uploads(b):\n",
        "    global test_images\n",
        "    test_images = []\n",
        "    if upload_widget.value:\n",
        "        for filename, file_info in upload_widget.value.items():\n",
        "            file_path = os.path.join(upload_dir, filename)\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(file_info['content'])\n",
        "            test_images.append(file_path)\n",
        "        print(f\"Uploaded {len(test_images)} image(s)\")\n",
        "\n",
        "upload_widget = FileUpload(accept='.jpg,.jpeg,.png', multiple=True)\n",
        "upload_button = Button(description='Upload', button_style='success')\n",
        "upload_button.on_click(process_uploads)\n",
        "\n",
        "display(upload_widget)\n",
        "display(upload_button)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "C5_VW7VceI5W"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "results = []\n",
        "\n",
        "def visualize_prediction_with_cam(img_path, model):\n",
        "\n",
        "    # ================= 1. PREDICT =================\n",
        "    pred, conf, probs = predict_tta(img_path, model)\n",
        "    print(f\"\\nPrediction  {pred} ({conf*100:.1f}%)\\n\")\n",
        "\n",
        "    # ================= 2. PREPROCESS =================\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    cropped = crop_eye_region_bgr(img_bgr)\n",
        "    enhanced = apply_clahe_rgb(cropped)\n",
        "    img_rgb = cv2.cvtColor(enhanced, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # model input\n",
        "    inp = val_aug(image=img_rgb)['image'].unsqueeze(0).to(device)\n",
        "\n",
        "    # ================= 3. GRAD-CAM =================\n",
        "    heat = cam(inp)\n",
        "    heatmap = cv2.applyColorMap((heat * 255).astype(\"uint8\"), cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "    heatmap = cv2.resize(heatmap, (img_rgb.shape[1], img_rgb.shape[0]))\n",
        "\n",
        "    # overlay\n",
        "    overlay = (0.55 * img_rgb + 0.45 * heatmap).astype(\"uint8\")\n",
        "\n",
        "    # ================= 4. SHARP DISPLAY IMAGES =================\n",
        "    display_img = cv2.resize(img_rgb, (400, 400), interpolation=cv2.INTER_NEAREST)\n",
        "    display_overlay = cv2.resize(overlay, (400, 400), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # ================= 5. PLOT (EYE  OVERLAY  CONFIDENCE) =================\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # 1. Eye Image\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(display_img)\n",
        "    plt.title(\"Eye (CLAHE)\", fontsize=12)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # 2. Overlay\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(display_overlay)\n",
        "    plt.title(\"Grad-CAM Overlay\", fontsize=12)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # 3. Confidence Plot\n",
        "    plt.subplot(1, 3, 3)\n",
        "    bars = plt.bar(CLASS_FILTER, probs * 100)\n",
        "    plt.ylabel(\"Confidence (%)\", fontsize=12)\n",
        "    plt.title(f\"Prediction: {pred}\", fontsize=12)\n",
        "    plt.ylim(0, 100)\n",
        "\n",
        "    for bar, p in zip(bars, probs):\n",
        "        plt.text(bar.get_x() + bar.get_width() / 2,\n",
        "                 p * 100 + 2,\n",
        "                 f\"{p*100:.1f}%\",\n",
        "                 ha='center',\n",
        "                 fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ================ SAVE RESULT =================\n",
        "    result = {\"filename\": img_path, \"prediction\": pred, \"confidence\": float(conf)}\n",
        "    for cls, p in zip(CLASS_FILTER, probs):\n",
        "        result[f\"prob_{cls}\"] = float(p)\n",
        "    results.append(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "XJYNFbAveNFY",
        "outputId": "f67fa751-165f-4b24-cfee-c4a0825ee43e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No test images specified. Set 'test_images' list in the previous cell to visualize predictions.\n"
          ]
        }
      ],
      "source": [
        "# Process uploaded images\n",
        "# Equivalent to: for fname in uploaded.keys(): visualize_prediction_with_cam(fname, model)\n",
        "if test_images:\n",
        "    for fname in test_images:\n",
        "        if os.path.exists(fname):\n",
        "            visualize_prediction_with_cam(fname, model)\n",
        "        else:\n",
        "            print(f\" Warning: File not found: {fname}\")\n",
        "    print(\"\\n Done!\")\n",
        "else:\n",
        "    print(\" No images uploaded!\")\n",
        "    print(\"Please upload images using the file upload widget in the previous cell.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MOm_RPBYpi3",
        "outputId": "b86a281e-f1ae-45c1-e8a2-a727cacae2b6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "uZ1tAaPVY8sg",
        "outputId": "4b93c7c1-60b8-47d4-9346-3b727e1013d3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
